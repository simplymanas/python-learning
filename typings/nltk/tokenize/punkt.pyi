"""
This type stub file was generated by pyright.
"""

import re
from nltk.compat import python_2_unicode_compatible
from nltk.tokenize.api import TokenizerI
from typing import Any, Optional

r"""
Punkt Sentence Tokenizer

This tokenizer divides a text into a list of sentences
by using an unsupervised algorithm to build a model for abbreviation
words, collocations, and words that start sentences.  It must be
trained on a large collection of plaintext in the target language
before it can be used.

The NLTK data package includes a pre-trained Punkt tokenizer for
English.

    >>> import nltk.data
    >>> text = '''
    ... Punkt knows that the periods in Mr. Smith and Johann S. Bach
    ... do not mark sentence boundaries.  And sometimes sentences
    ... can start with non-capitalized words.  i is a good variable
    ... name.
    ... '''
    >>> sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')
    >>> print('\n-----\n'.join(sent_detector.tokenize(text.strip())))
    Punkt knows that the periods in Mr. Smith and Johann S. Bach
    do not mark sentence boundaries.
    -----
    And sometimes sentences
    can start with non-capitalized words.
    -----
    i is a good variable
    name.

(Note that whitespace from the original text, including newlines, is
retained in the output.)

Punctuation following sentences is also included by default
(from NLTK 3.0 onwards). It can be excluded with the realign_boundaries
flag.

    >>> text = '''
    ... (How does it deal with this parenthesis?)  "It should be part of the
    ... previous sentence." "(And the same with this one.)" ('And this one!')
    ... "('(And (this)) '?)" [(and this. )]
    ... '''
    >>> print('\n-----\n'.join(
    ...     sent_detector.tokenize(text.strip())))
    (How does it deal with this parenthesis?)
    -----
    "It should be part of the
    previous sentence."
    -----
    "(And the same with this one.)"
    -----
    ('And this one!')
    -----
    "('(And (this)) '?)"
    -----
    [(and this. )]
    >>> print('\n-----\n'.join(
    ...     sent_detector.tokenize(text.strip(), realign_boundaries=False)))
    (How does it deal with this parenthesis?
    -----
    )  "It should be part of the
    previous sentence.
    -----
    " "(And the same with this one.
    -----
    )" ('And this one!
    -----
    ')
    "('(And (this)) '?
    -----
    )" [(and this.
    -----
    )]

However, Punkt is designed to learn parameters (a list of abbreviations, etc.)
unsupervised from a corpus similar to the target domain. The pre-packaged models
may therefore be unsuitable: use ``PunktSentenceTokenizer(text)`` to learn
parameters from the given text.

:class:`.PunktTrainer` learns parameters such as a list of abbreviations
(without supervision) from portions of text. Using a ``PunktTrainer`` directly
allows for incremental training and modification of the hyper-parameters used
to decide what is considered an abbreviation, etc.

The algorithm for this tokenizer is described in::

  Kiss, Tibor and Strunk, Jan (2006): Unsupervised Multilingual Sentence
    Boundary Detection.  Computational Linguistics 32: 485-525.
"""
_ORTHO_BEG_UC = 1 << 1
_ORTHO_MID_UC = 1 << 2
_ORTHO_UNK_UC = 1 << 3
_ORTHO_BEG_LC = 1 << 4
_ORTHO_MID_LC = 1 << 5
_ORTHO_UNK_LC = 1 << 6
_ORTHO_UC = _ORTHO_BEG_UC + _ORTHO_MID_UC + _ORTHO_UNK_UC
_ORTHO_LC = _ORTHO_BEG_LC + _ORTHO_MID_LC + _ORTHO_UNK_LC
_ORTHO_MAP = { ('initial', 'upper'): _ORTHO_BEG_UC,('internal', 'upper'): _ORTHO_MID_UC,('unknown', 'upper'): _ORTHO_UNK_UC,('initial', 'lower'): _ORTHO_BEG_LC,('internal', 'lower'): _ORTHO_MID_LC,('unknown', 'lower'): _ORTHO_UNK_LC }
REASON_DEFAULT_DECISION = 'default decision'
REASON_KNOWN_COLLOCATION = 'known collocation (both words)'
REASON_ABBR_WITH_ORTHOGRAPHIC_HEURISTIC = 'abbreviation + orthographic heuristic'
REASON_ABBR_WITH_SENTENCE_STARTER = 'abbreviation + frequent sentence starter'
REASON_INITIAL_WITH_ORTHOGRAPHIC_HEURISTIC = 'initial + orthographic heuristic'
REASON_NUMBER_WITH_ORTHOGRAPHIC_HEURISTIC = 'initial + orthographic heuristic'
REASON_INITIAL_WITH_SPECIAL_ORTHOGRAPHIC_HEURISTIC = 'initial + special orthographic heuristic'
class PunktLanguageVars(object):
    """
    Stores variables, mostly regular expressions, which may be
    language-dependent for correct application of the algorithm.
    An extension of this class may modify its properties to suit
    a language other than English; an instance can then be passed
    as an argument to PunktSentenceTokenizer and PunktTrainer
    constructors.
    """
    __slots__ = ...
    def __getstate__(self):
        ...
    
    def __setstate__(self, state):
        ...
    
    sent_end_chars = ...
    @property
    def _re_sent_end_chars(self):
        ...
    
    internal_punctuation = ...
    re_boundary_realignment = ...
    _re_word_start = ...
    _re_non_word_chars = ...
    _re_multi_char_punct = ...
    _word_tokenize_fmt = ...
    def _word_tokenizer_re(self):
        """Compiles and returns a regular expression for word tokenization"""
        ...
    
    def word_tokenize(self, s):
        """Tokenize a string to split off punctuation other than periods"""
        ...
    
    _period_context_fmt = ...
    def period_context_re(self):
        """Compiles and returns a regular expression to find contexts
        including possible sentence boundaries."""
        ...
    


_re_non_punct = re.compile(r'[^\W\d]', re.UNICODE)
def _pair_iter(it):
    """
    Yields pairs of tokens from the given iterator such that each input
    token will appear as the first element in a yielded tuple. The last
    pair will have None as its second element.
    """
    ...

class PunktParameters(object):
    """Stores data used to perform sentence boundary detection with Punkt."""
    def __init__(self):
        self.abbrev_types = ...
        self.collocations = ...
        self.sent_starters = ...
        self.ortho_context = ...
    
    def clear_abbrevs(self):
        self.abbrev_types = ...
    
    def clear_collocations(self):
        self.collocations = ...
    
    def clear_sent_starters(self):
        self.sent_starters = ...
    
    def clear_ortho_context(self):
        self.ortho_context = ...
    
    def add_ortho_context(self, typ, flag):
        ...
    
    def _debug_ortho_context(self, typ):
        ...
    


@python_2_unicode_compatible
class PunktToken(object):
    """Stores a token of text with annotations produced during
    sentence boundary detection."""
    _properties = ...
    __slots__ = ...
    def __init__(self, tok, **params):
        self.tok = ...
        self.type = ...
        self.period_final = ...
    
    _RE_ELLIPSIS = ...
    _RE_NUMERIC = ...
    _RE_INITIAL = ...
    _RE_ALPHA = ...
    def _get_type(self, tok):
        """Returns a case-normalized representation of the token."""
        ...
    
    @property
    def type_no_period(self):
        """
        The type with its final period removed if it has one.
        """
        ...
    
    @property
    def type_no_sentperiod(self):
        """
        The type with its final period removed if it is marked as a
        sentence break.
        """
        ...
    
    @property
    def first_upper(self):
        """True if the token's first character is uppercase."""
        ...
    
    @property
    def first_lower(self):
        """True if the token's first character is lowercase."""
        ...
    
    @property
    def first_case(self):
        ...
    
    @property
    def is_ellipsis(self):
        """True if the token text is that of an ellipsis."""
        ...
    
    @property
    def is_number(self):
        """True if the token text is that of a number."""
        ...
    
    @property
    def is_initial(self):
        """True if the token text is that of an initial."""
        ...
    
    @property
    def is_alpha(self):
        """True if the token text is all alphabetic."""
        ...
    
    @property
    def is_non_punct(self):
        """True if the token is either a number or is alphabetic."""
        ...
    
    def __repr__(self):
        """
        A string representation of the token that can reproduce it
        with eval(), which lists all the token's non-default
        annotations.
        """
        ...
    
    def __str__(self):
        """
        A string representation akin to that used by Kiss and Strunk.
        """
        ...
    


class PunktBaseClass(object):
    """
    Includes common components of PunktTrainer and PunktSentenceTokenizer.
    """
    def __init__(self, lang_vars: Optional[Any] = ..., token_cls=..., params: Optional[Any] = ...):
        ...
    
    def _tokenize_words(self, plaintext):
        """
        Divide the given text into tokens, using the punkt word
        segmentation regular expression, and generate the resulting list
        of tokens augmented as three-tuples with two boolean values for whether
        the given token occurs at the start of a paragraph or a new line,
        respectively.
        """
        ...
    
    def _annotate_first_pass(self, tokens):
        """
        Perform the first pass of annotation, which makes decisions
        based purely based on the word type of each word:

          - '?', '!', and '.' are marked as sentence breaks.
          - sequences of two or more periods are marked as ellipsis.
          - any word ending in '.' that's a known abbreviation is
            marked as an abbreviation.
          - any other word ending in '.' is marked as a sentence break.

        Return these annotations as a tuple of three sets:

          - sentbreak_toks: The indices of all sentence breaks.
          - abbrev_toks: The indices of all abbreviations.
          - ellipsis_toks: The indices of all ellipsis marks.
        """
        ...
    
    def _first_pass_annotation(self, aug_tok):
        """
        Performs type-based annotation on a single token.
        """
        ...
    


class PunktTrainer(PunktBaseClass):
    """Learns parameters used in Punkt sentence boundary detection."""
    def __init__(self, train_text: Optional[Any] = ..., verbose: bool = ..., lang_vars: Optional[Any] = ..., token_cls=...):
        ...
    
    def get_params(self):
        """
        Calculates and returns parameters for sentence boundary detection as
        derived from training."""
        ...
    
    ABBREV = ...
    IGNORE_ABBREV_PENALTY = ...
    ABBREV_BACKOFF = ...
    COLLOCATION = ...
    SENT_STARTER = ...
    INCLUDE_ALL_COLLOCS = ...
    INCLUDE_ABBREV_COLLOCS = ...
    MIN_COLLOC_FREQ = ...
    def train(self, text, verbose: bool = ..., finalize: bool = ...):
        """
        Collects training data from a given text. If finalize is True, it
        will determine all the parameters for sentence boundary detection. If
        not, this will be delayed until get_params() or finalize_training() is
        called. If verbose is True, abbreviations found will be listed.
        """
        ...
    
    def train_tokens(self, tokens, verbose: bool = ..., finalize: bool = ...):
        """
        Collects training data from a given list of tokens.
        """
        ...
    
    def _train_tokens(self, tokens, verbose):
        ...
    
    def _unique_types(self, tokens):
        ...
    
    def finalize_training(self, verbose: bool = ...):
        """
        Uses data that has been gathered in training to determine likely
        collocations and sentence starters.
        """
        ...
    
    def freq_threshold(self, ortho_thresh=..., type_thresh=..., colloc_thres=..., sentstart_thresh=...):
        """
        Allows memory use to be reduced after much training by removing data
        about rare tokens that are unlikely to have a statistical effect with
        further training. Entries occurring above the given thresholds will be
        retained.
        """
        ...
    
    def _freq_threshold(self, fdist, threshold):
        """
        Returns a FreqDist containing only data with counts below a given
        threshold, as well as a mapping (None -> count_removed).
        """
        ...
    
    def _get_orthography_data(self, tokens):
        """
        Collect information about whether each token type occurs
        with different case patterns (i) overall, (ii) at
        sentence-initial positions, and (iii) at sentence-internal
        positions.
        """
        ...
    
    def _reclassify_abbrev_types(self, types):
        """
        (Re)classifies each given token if
          - it is period-final and not a known abbreviation; or
          - it is not period-final and is otherwise a known abbreviation
        by checking whether its previous classification still holds according
        to the heuristics of section 3.
        Yields triples (abbr, score, is_add) where abbr is the type in question,
        score is its log-likelihood with penalties applied, and is_add specifies
        whether the present type is a candidate for inclusion or exclusion as an
        abbreviation, such that:
          - (is_add and score >= 0.3)    suggests a new abbreviation; and
          - (not is_add and score < 0.3) suggests excluding an abbreviation.
        """
        ...
    
    def find_abbrev_types(self):
        """
        Recalculates abbreviations given type frequencies, despite no prior
        determination of abbreviations.
        This fails to include abbreviations otherwise found as "rare".
        """
        ...
    
    def _is_rare_abbrev_type(self, cur_tok, next_tok):
        """
        A word type is counted as a rare abbreviation if...
          - it's not already marked as an abbreviation
          - it occurs fewer than ABBREV_BACKOFF times
          - either it is followed by a sentence-internal punctuation
            mark, *or* it is followed by a lower-case word that
            sometimes appears with upper case, but never occurs with
            lower case at the beginning of sentences.
        """
        ...
    
    @staticmethod
    def _dunning_log_likelihood(count_a, count_b, count_ab, N):
        """
        A function that calculates the modified Dunning log-likelihood
        ratio scores for abbreviation candidates.  The details of how
        this works is available in the paper.
        """
        ...
    
    @staticmethod
    def _col_log_likelihood(count_a, count_b, count_ab, N):
        """
        A function that will just compute log-likelihood estimate, in
        the original paper it's described in algorithm 6 and 7.

        This *should* be the original Dunning log-likelihood values,
        unlike the previous log_l function where it used modified
        Dunning log-likelihood values
        """
        ...
    
    def _is_potential_collocation(self, aug_tok1, aug_tok2):
        """
        Returns True if the pair of tokens may form a collocation given
        log-likelihood statistics.
        """
        ...
    
    def _find_collocations(self):
        """
        Generates likely collocations and their log-likelihood.
        """
        ...
    
    def _is_potential_sent_starter(self, cur_tok, prev_tok):
        """
        Returns True given a token and the token that preceds it if it
        seems clear that the token is beginning a sentence.
        """
        ...
    
    def _find_sent_starters(self):
        """
        Uses collocation heuristics for each candidate token to
        determine if it frequently starts sentences.
        """
        ...
    
    def _get_sentbreak_count(self, tokens):
        """
        Returns the number of sentence breaks marked in a given set of
        augmented tokens.
        """
        ...
    


class PunktSentenceTokenizer(PunktBaseClass, TokenizerI):
    """
    A sentence tokenizer which uses an unsupervised algorithm to build
    a model for abbreviation words, collocations, and words that start
    sentences; and then uses that model to find sentence boundaries.
    This approach has been shown to work well for many European
    languages.
    """
    def __init__(self, train_text: Optional[Any] = ..., verbose: bool = ..., lang_vars: Optional[Any] = ..., token_cls=...):
        """
        train_text can either be the sole training text for this sentence
        boundary detector, or can be a PunktParameters object.
        """
        ...
    
    def train(self, train_text, verbose: bool = ...):
        """
        Derives parameters from a given training text, or uses the parameters
        given. Repeated calls to this method destroy previous parameters. For
        incremental training, instantiate a separate PunktTrainer instance.
        """
        ...
    
    def tokenize(self, text, realign_boundaries: bool = ...):
        """
        Given a text, returns a list of the sentences in that text.
        """
        ...
    
    def debug_decisions(self, text):
        """
        Classifies candidate periods as sentence breaks, yielding a dict for
        each that may be used to understand why the decision was made.

        See format_debug_decision() to help make this output readable.
        """
        ...
    
    def span_tokenize(self, text, realign_boundaries: bool = ...):
        """
        Given a text, generates (start, end) spans of sentences
        in the text.
        """
        ...
    
    def sentences_from_text(self, text, realign_boundaries: bool = ...):
        """
        Given a text, generates the sentences in that text by only
        testing candidate sentence breaks. If realign_boundaries is
        True, includes in the sentence closing punctuation that
        follows the period.
        """
        ...
    
    def _slices_from_text(self, text):
        ...
    
    def _realign_boundaries(self, text, slices):
        """
        Attempts to realign punctuation that falls after the period but
        should otherwise be included in the same sentence.

        For example: "(Sent1.) Sent2." will otherwise be split as::

            ["(Sent1.", ") Sent1."].

        This method will produce::

            ["(Sent1.)", "Sent2."].
        """
        ...
    
    def text_contains_sentbreak(self, text):
        """
        Returns True if the given text includes a sentence break.
        """
        ...
    
    def sentences_from_text_legacy(self, text):
        """
        Given a text, generates the sentences in that text. Annotates all
        tokens, rather than just those with possible sentence breaks. Should
        produce the same results as ``sentences_from_text``.
        """
        ...
    
    def sentences_from_tokens(self, tokens):
        """
        Given a sequence of tokens, generates lists of tokens, each list
        corresponding to a sentence.
        """
        ...
    
    def _annotate_tokens(self, tokens):
        """
        Given a set of tokens augmented with markers for line-start and
        paragraph-start, returns an iterator through those tokens with full
        annotation including predicted sentence breaks.
        """
        ...
    
    def _build_sentence_list(self, text, tokens):
        """
        Given the original text and the list of augmented word tokens,
        construct and return a tokenized list of sentence strings.
        """
        ...
    
    def dump(self, tokens):
        ...
    
    PUNCTUATION = ...
    def _annotate_second_pass(self, tokens):
        """
        Performs a token-based classification (section 4) over the given
        tokens, making use of the orthographic heuristic (4.1.1), collocation
        heuristic (4.1.2) and frequent sentence starter heuristic (4.1.3).
        """
        ...
    
    def _second_pass_annotation(self, aug_tok1, aug_tok2):
        """
        Performs token-based classification over a pair of contiguous tokens
        updating the first.
        """
        ...
    
    def _ortho_heuristic(self, aug_tok):
        """
        Decide whether the given token is the first token in a sentence.
        """
        ...
    


DEBUG_DECISION_FMT = '''Text: %(text)r (at offset %(period_index)d)
Sentence break? %(break_decision)s (%(reason)s)
Collocation? %(collocation)s
%(type1)r:
    known abbreviation: %(type1_in_abbrs)s
    is initial: %(type1_is_initial)s
%(type2)r:
    known sentence starter: %(type2_is_sent_starter)s
    orthographic heuristic suggests is a sentence starter? %(type2_ortho_heuristic)s
    orthographic contexts in training: %(type2_ortho_contexts)s
'''
def format_debug_decision(d):
    ...

def demo(text, tok_cls=..., train_cls=...):
    """Builds a punkt model and applies it to the same text"""
    ...

