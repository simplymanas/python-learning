"""
This type stub file was generated by pyright.
"""

from nltk.tokenize.api import TokenizerI

"""
The tok-tok tokenizer is a simple, general tokenizer, where the input has one
sentence per line; thus only final period is tokenized.

Tok-tok has been tested on, and gives reasonably good results for English,
Persian, Russian, Czech, French, German, Vietnamese, Tajik, and a few others.
The input should be in UTF-8 encoding.

Reference:
Jon Dehdari. 2014. A Neurophysiologically-Inspired Statistical Language
Model (Doctoral dissertation). Columbus, OH, USA: The Ohio State University.
"""
class ToktokTokenizer(TokenizerI):
    """
    This is a Python port of the tok-tok.pl from
    https://github.com/jonsafari/tok-tok/blob/master/tok-tok.pl

    >>> toktok = ToktokTokenizer()
    >>> text = u'Is 9.5 or 525,600 my favorite number?'
    >>> print (toktok.tokenize(text, return_str=True))
    Is 9.5 or 525,600 my favorite number ?
    >>> text = u'The https://github.com/jonsafari/tok-tok/blob/master/tok-tok.pl is a website with/and/or slashes and sort of weird : things'
    >>> print (toktok.tokenize(text, return_str=True))
    The https://github.com/jonsafari/tok-tok/blob/master/tok-tok.pl is a website with/and/or slashes and sort of weird : things
    >>> text = u'\xa1This, is a sentence with weird\xbb symbols\u2026 appearing everywhere\xbf'
    >>> expected = u'\xa1 This , is a sentence with weird \xbb symbols \u2026 appearing everywhere \xbf'
    >>> assert toktok.tokenize(text, return_str=True) == expected
    >>> toktok.tokenize(text) == [u'\xa1', u'This', u',', u'is', u'a', u'sentence', u'with', u'weird', u'\xbb', u'symbols', u'\u2026', u'appearing', u'everywhere', u'\xbf']
    True
    """
    NON_BREAKING = ...
    FUNKY_PUNCT_1 = ...
    FUNKY_PUNCT_2 = ...
    EN_EM_DASHES = ...
    AMPERCENT = ...
    TAB = ...
    PIPE = ...
    COMMA_IN_NUM = ...
    PROB_SINGLE_QUOTES = ...
    STUPID_QUOTES_1 = ...
    STUPID_QUOTES_2 = ...
    FINAL_PERIOD_1 = ...
    FINAL_PERIOD_2 = ...
    MULTI_COMMAS = ...
    MULTI_DASHES = ...
    MULTI_DOTS = ...
    OPEN_PUNCT = ...
    CLOSE_PUNCT = ...
    CURRENCY_SYM = ...
    OPEN_PUNCT_RE = ...
    CLOSE_PUNCT_RE = ...
    CURRENCY_SYM_RE = ...
    URL_FOE_1 = ...
    URL_FOE_2 = ...
    URL_FOE_3 = ...
    URL_FOE_4 = ...
    LSTRIP = ...
    RSTRIP = ...
    ONE_SPACE = ...
    TOKTOK_REGEXES = ...
    def tokenize(self, text, return_str: bool = ...):
        ...
    


